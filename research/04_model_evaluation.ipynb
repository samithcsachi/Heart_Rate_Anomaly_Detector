{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c7a246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4efb50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "964eaeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc73b849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67d5267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Dict[str, Path]   \n",
    "    model_path: Dict[str, Path]       \n",
    "    report_path: Dict[str, Path]      \n",
    "    target_columns: Dict[str, str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04dc442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Heart_Rate_Anomaly_Detector.constants import *\n",
    "from Heart_Rate_Anomaly_Detector.utils.common import read_yaml, create_directories\n",
    "from pathlib import Path\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78fa3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH\n",
    "    ):\n",
    "        # Load YAML files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        \n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "       \n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            test_data_path={k: Path(v) for k, v in config.test_data_path.items()},\n",
    "            model_path={k: Path(v) for k, v in config.model_path.items()},\n",
    "            report_path={k: Path(v) for k, v in config.report_path.items()},\n",
    "            target_columns={k: v for k, v in config.target_columns.items()}\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d188256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from Heart_Rate_Anomaly_Detector import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23cb6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        self.config = config\n",
    "        self.predictions: Dict[str, np.ndarray] = {}\n",
    "        self.actuals: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    def load_model_and_artifacts(self, model_key: str):\n",
    "        model_path = self.config.model_path[model_key]\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "        model_artifacts = joblib.load(model_path)\n",
    "        logger.info(f\"Loaded model artifacts for {model_key}: {model_artifacts.get('model_type', 'Unknown')}\")\n",
    "        return model_artifacts\n",
    "\n",
    "    def load_test_data(self, model_key: str):\n",
    "        test_path = self.config.test_data_path[model_key]\n",
    "        if not os.path.exists(test_path):\n",
    "            raise FileNotFoundError(f\"Test data not found: {test_path}\")\n",
    "        \n",
    "        if str(test_path).endswith(\".csv\"):\n",
    "            df = pd.read_csv(test_path)\n",
    "        elif str(test_path).endswith(\".joblib\"):\n",
    "            df = joblib.load(test_path)\n",
    "            if isinstance(df, dict):\n",
    "                X_test = df.get('X_test')\n",
    "                y_test = df.get('y_test')\n",
    "                if X_test is None or y_test is None:\n",
    "                    raise ValueError(f\"Joblib test data must contain 'X_test' and 'y_test'\")\n",
    "                return X_test, y_test\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported test file format: {test_path}\")\n",
    "        \n",
    "        target_col = self.config.target_columns[model_key]\n",
    "        if target_col not in df.columns:\n",
    "            raise ValueError(f\"Target column '{target_col}' not found in test data\")\n",
    "        X_test = df.drop(columns=[target_col])\n",
    "        y_test = df[target_col]\n",
    "        return X_test, y_test\n",
    "\n",
    "    def validate_feature_compatibility(self, X_test, expected_features):\n",
    "        X_test = X_test.reindex(columns=expected_features, fill_value=0)\n",
    "        logger.info(f\"Feature validation completed. Final shape: {X_test.shape}\")\n",
    "        return X_test\n",
    "\n",
    "    def calculate_metrics(self, y_true, y_pred):\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        mape = np.mean(np.abs((y_true - y_pred) / np.where(y_true == 0, 1e-8, y_true))) * 100\n",
    "\n",
    "        residuals = y_true - y_pred\n",
    "        mean_residual = np.mean(residuals)\n",
    "        std_residual = np.std(residuals)\n",
    "\n",
    "        pred_range = y_pred.max() - y_pred.min()\n",
    "        actual_range = y_true.max() - y_true.min()\n",
    "        range_coverage = (pred_range / actual_range * 100) if actual_range > 0 else 0\n",
    "\n",
    "        metrics = {\n",
    "            \"mae\": float(mae),\n",
    "            \"rmse\": float(rmse),\n",
    "            \"r2\": float(r2),\n",
    "            \"mape\": float(mape),\n",
    "            \"mean_residual\": float(mean_residual),\n",
    "            \"std_residual\": float(std_residual),\n",
    "            \"range_coverage\": float(range_coverage),\n",
    "            \"n_samples\": len(y_true),\n",
    "            \"prediction_stats\": {\n",
    "                \"mean\": float(np.mean(y_pred)),\n",
    "                \"std\": float(np.std(y_pred)),\n",
    "                \"min\": float(np.min(y_pred)),\n",
    "                \"max\": float(np.max(y_pred)),\n",
    "            },\n",
    "            \"actual_stats\": {\n",
    "                \"mean\": float(np.mean(y_true)),\n",
    "                \"std\": float(np.std(y_true)),\n",
    "                \"min\": float(np.min(y_true)),\n",
    "                \"max\": float(np.max(y_true)),\n",
    "            }\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def save_results(self, model_key: str, metrics, model_artifacts):\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "\n",
    "        result_path = Path(self.config.report_path[model_key])\n",
    "        os.makedirs(result_path.parent, exist_ok=True)\n",
    "\n",
    "        full_results = {\n",
    "            \"model_key\": model_key,\n",
    "            \"model_info\": {\n",
    "                \"model_type\": model_artifacts.get(\"model_type\", \"Unknown\"),\n",
    "                \"target_column\": model_artifacts.get(\"target_column\", \"Unknown\"),\n",
    "                \"timestamp\": model_artifacts.get(\"timestamp\", \"Unknown\"),\n",
    "                \"feature_count\": len(model_artifacts.get(\"feature_columns\", []))\n",
    "            },\n",
    "            \"metrics\": metrics,\n",
    "            \"evaluation_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "        with open(result_path, \"w\") as f:\n",
    "            json.dump(full_results, f, indent=4)\n",
    "\n",
    "        logger.info(f\"Saved evaluation results for {model_key} at {result_path}\")\n",
    "\n",
    "    def evaluate_model(self, model_key: str):\n",
    "        logger.info(f\"Evaluating model: {model_key}\")\n",
    "        model_artifacts = self.load_model_and_artifacts(model_key)\n",
    "        model = model_artifacts[\"model\"]\n",
    "\n",
    "        X_test, y_test = self.load_test_data(model_key)\n",
    "        expected_features = model_artifacts.get(\"feature_columns\", [])\n",
    "        if expected_features:\n",
    "            X_test = self.validate_feature_compatibility(X_test, expected_features)\n",
    "\n",
    "        logger.info(\"Generating predictions...\")\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        self.predictions[model_key] = y_pred\n",
    "        self.actuals[model_key] = y_test\n",
    "\n",
    "        metrics = self.calculate_metrics(y_test, y_pred)\n",
    "        self.save_results(model_key, metrics, model_artifacts)\n",
    "        logger.info(f\"Evaluation completed for {model_key}\")\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_all(self):\n",
    "        results = {}\n",
    "        for model_key in self.config.model_path.keys():\n",
    "            results[model_key] = self.evaluate_model(model_key)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7769ee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-07 23:37:01,495: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-07 23:37:01,503: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-07 23:37:01,512: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-09-07 23:37:01,513: INFO: common: created directory at: artifacts]\n",
      "[2025-09-07 23:37:01,516: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "[2025-09-07 23:37:01,519: INFO: 2018659500: Evaluating model: heart_rate]\n",
      "[2025-09-07 23:37:09,179: INFO: 2018659500: Loaded model artifacts for heart_rate: RandomForestRegressor]\n",
      "[2025-09-07 23:37:09,314: INFO: 2018659500: Feature validation completed. Final shape: (75600, 30)]\n",
      "[2025-09-07 23:37:09,316: INFO: 2018659500: Generating predictions...]\n",
      "[2025-09-07 23:37:14,116: INFO: 2018659500: Saved evaluation results for heart_rate at artifacts\\model_evaluation\\heart_rate_report.json]\n",
      "[2025-09-07 23:37:14,116: INFO: 2018659500: Evaluation completed for heart_rate]\n",
      "[2025-09-07 23:37:14,292: INFO: 2018659500: Evaluating model: anomaly]\n",
      "[2025-09-07 23:37:15,935: INFO: 2018659500: Loaded model artifacts for anomaly: IsolationForest]\n",
      "[2025-09-07 23:37:16,020: INFO: 2018659500: Feature validation completed. Final shape: (75600, 20)]\n",
      "[2025-09-07 23:37:16,023: INFO: 2018659500: Generating predictions...]\n",
      "[2025-09-07 23:37:19,047: INFO: 2018659500: Saved evaluation results for anomaly at artifacts\\model_evaluation\\anomaly_report.json]\n",
      "[2025-09-07 23:37:19,051: INFO: 2018659500: Evaluation completed for anomaly]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluator = ModelEvaluation(config=model_evaluation_config)\n",
    "    evaluation_results = model_evaluator.evaluate_all()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
