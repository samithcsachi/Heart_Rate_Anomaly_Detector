{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbfdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd283915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35fde99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a90442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63aad981",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_heart_rate_data_path: Path\n",
    "    test_heart_rate_data_path: Path\n",
    "    train_is_anomaly_data_path: Path\n",
    "    test_is_anomaly_data_path: Path\n",
    "    data_transformation_dir: Path  \n",
    "    \n",
    "   \n",
    "    heart_rate_predictor_model_name: str\n",
    "    anomaly_detector_model_name: str\n",
    "    \n",
    "    \n",
    "    heart_rate_target_column: str\n",
    "    anomaly_target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876ada10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Heart_Rate_Anomaly_Detector.constants import *\n",
    "from Heart_Rate_Anomaly_Detector.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4672ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        \n",
    "        \n",
    "        heart_rate_params = self.params.HEART_RATE_PREDICTOR\n",
    "        anomaly_params = self.params.ANOMALY_DETECTOR\n",
    "        \n",
    "       \n",
    "        heart_rate_model_schema = self.schema.models.HeartRatePredictor\n",
    "        anomaly_model_schema = self.schema.models.AnomalyDetector\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_heart_rate_data_path=Path(config.data_path.train_heart_rate),\n",
    "            test_heart_rate_data_path=Path(config.data_path.test_heart_rate),\n",
    "            train_is_anomaly_data_path=Path(config.data_path.train_is_anomaly),\n",
    "            test_is_anomaly_data_path=Path(config.data_path.test_is_anomaly),\n",
    "            data_transformation_dir=Path(self.config.data_transformation.root_dir),\n",
    "            \n",
    "         \n",
    "            heart_rate_predictor_model_name=config.model_name.heart_rate_predictor,\n",
    "            anomaly_detector_model_name=config.model_name.anomaly_detector,\n",
    "            \n",
    "           \n",
    "            heart_rate_target_column=heart_rate_model_schema.target_column,\n",
    "            anomaly_target_column=anomaly_model_schema.target_column\n",
    "        )\n",
    "        return model_trainer_config\n",
    "    \n",
    "    def get_heart_rate_features(self) -> list:\n",
    "       \n",
    "        return self.schema.models.HeartRatePredictor.features\n",
    "    \n",
    "    def get_anomaly_features(self) -> list:\n",
    "       \n",
    "        return self.schema.models.AnomalyDetector.features\n",
    "    \n",
    "    def get_column_dtypes(self) -> dict:\n",
    "        \n",
    "        return self.schema.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46fc7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from Heart_Rate_Anomaly_Detector import logger\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a9553cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config, schema_config, params, model_name):\n",
    "        self.config = config\n",
    "        self.schema_config = schema_config\n",
    "        self.params = params\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.encoder = None\n",
    "\n",
    "        self.target_column = schema_config.models[model_name].target_column\n",
    "        self.features = schema_config.models[model_name].features\n",
    "\n",
    "    def load_data(self):\n",
    "        data_paths = {\n",
    "            \"HeartRatePredictor\": (\n",
    "                self.config.train_heart_rate_data_path,\n",
    "                self.config.test_heart_rate_data_path\n",
    "            ),\n",
    "            \"AnomalyDetector\": (\n",
    "                self.config.train_is_anomaly_data_path,\n",
    "                self.config.test_is_anomaly_data_path\n",
    "            )\n",
    "        }\n",
    "\n",
    "        if self.model_name not in data_paths:\n",
    "            raise ValueError(f\"Unknown model name: {self.model_name}\")\n",
    "\n",
    "        train_path, test_path = data_paths[self.model_name]\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "\n",
    "        logger.info(f\"Data loaded for {self.model_name} - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
    "        return train_data, test_data\n",
    "\n",
    "    def _encode_categorical_features(self, train_x, test_x, train_y):\n",
    "        cat_cols = train_x.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "        exclude_cols = ['user_id', 'email', 'name']\n",
    "        id_cols_to_drop = [col for col in exclude_cols if col in train_x.columns]\n",
    "        if id_cols_to_drop:\n",
    "            logger.info(f\"Dropping ID columns: {id_cols_to_drop}\")\n",
    "            train_x = train_x.drop(columns=id_cols_to_drop)\n",
    "            test_x = test_x.drop(columns=id_cols_to_drop)\n",
    "            cat_cols = [col for col in cat_cols if col not in id_cols_to_drop]\n",
    "\n",
    "        if not cat_cols:\n",
    "            return train_x, test_x\n",
    "\n",
    "        logger.info(f\"Processing categorical columns: {cat_cols}\")\n",
    "        cardinality_info = {col: train_x[col].nunique() for col in cat_cols}\n",
    "        for col, count in cardinality_info.items():\n",
    "            logger.info(f\"Column '{col}': {count} unique values\")\n",
    "\n",
    "        high_card_cols = [col for col, count in cardinality_info.items() if count > 20]\n",
    "        low_card_cols = [col for col, count in cardinality_info.items() if count <= 20]\n",
    "\n",
    "        if high_card_cols:\n",
    "            train_x, test_x = self._apply_target_encoding(train_x, test_x, train_y, high_card_cols)\n",
    "\n",
    "        if low_card_cols:\n",
    "            train_x, test_x = self._apply_onehot_encoding(train_x, test_x, low_card_cols)\n",
    "\n",
    "        return train_x, test_x\n",
    "\n",
    "    def _apply_target_encoding(self, train_x, test_x, train_y, cols):\n",
    "        logger.info(f\"Applying target encoding to: {cols}\")\n",
    "        target_mean = train_y.mean()\n",
    "\n",
    "        for col in cols:\n",
    "            temp_df = pd.DataFrame({\"category\": train_x[col], \"target\": train_y.values})\n",
    "            encoding_map = temp_df.groupby(\"category\")[\"target\"].mean().to_dict()\n",
    "\n",
    "            train_x[f\"{col}_encoded\"] = train_x[col].map(encoding_map).fillna(target_mean)\n",
    "            test_x[f\"{col}_encoded\"] = test_x[col].map(encoding_map).fillna(target_mean)\n",
    "\n",
    "            train_x = train_x.drop(columns=[col])\n",
    "            test_x = test_x.drop(columns=[col])\n",
    "\n",
    "        return train_x, test_x\n",
    "\n",
    "    def _apply_onehot_encoding(self, train_x, test_x, cols):\n",
    "        logger.info(f\"Applying one-hot encoding to: {cols}\")\n",
    "        expected_features = sum(train_x[col].nunique() - 1 for col in cols)\n",
    "\n",
    "        if expected_features > 500:\n",
    "            logger.warning(f\"Too many expected features ({expected_features}), switching to frequency encoding\")\n",
    "            return self._apply_frequency_encoding(train_x, test_x, cols)\n",
    "\n",
    "        try:\n",
    "            self.encoder = OneHotEncoder(drop=\"first\", sparse_output=False, handle_unknown=\"ignore\")\n",
    "            train_encoded = self.encoder.fit_transform(train_x[cols])\n",
    "            test_encoded = self.encoder.transform(test_x[cols])\n",
    "\n",
    "            feature_names = self.encoder.get_feature_names_out(cols)\n",
    "            train_encoded_df = pd.DataFrame(train_encoded, columns=feature_names, index=train_x.index)\n",
    "            test_encoded_df = pd.DataFrame(test_encoded, columns=feature_names, index=test_x.index)\n",
    "\n",
    "            remaining_cols = [col for col in train_x.columns if col not in cols]\n",
    "            train_x = pd.concat([train_x[remaining_cols], train_encoded_df], axis=1)\n",
    "            test_x = pd.concat([test_x[remaining_cols], test_encoded_df], axis=1)\n",
    "\n",
    "        except MemoryError:\n",
    "            logger.error(\"Memory error in one-hot encoding, falling back to frequency encoding\")\n",
    "            return self._apply_frequency_encoding(train_x, test_x, cols)\n",
    "\n",
    "        return train_x, test_x\n",
    "\n",
    "    def _apply_frequency_encoding(self, train_x, test_x, cols):\n",
    "        logger.info(f\"Applying frequency encoding to: {cols}\")\n",
    "        for col in cols:\n",
    "            freq_map = train_x[col].value_counts(normalize=True).to_dict()\n",
    "            train_x[f\"{col}_freq\"] = train_x[col].map(freq_map)\n",
    "            test_x[f\"{col}_freq\"] = test_x[col].map(freq_map).fillna(0)\n",
    "            train_x = train_x.drop(columns=[col])\n",
    "            test_x = test_x.drop(columns=[col])\n",
    "        return train_x, test_x\n",
    "\n",
    "    def prepare_features(self, train_data, test_data):\n",
    "        available_features = [col for col in self.features if col in train_data.columns]\n",
    "\n",
    "        train_x = train_data[available_features].copy()\n",
    "        test_x = test_data[available_features].copy()\n",
    "\n",
    "        train_y = train_data[self.target_column]\n",
    "        test_y = test_data[self.target_column]\n",
    "\n",
    "        train_x, test_x = self._encode_categorical_features(train_x, test_x, train_y)\n",
    "        train_x, test_x = self._final_cleanup(train_x, test_x)\n",
    "\n",
    "        if self.model_name == \"AnomalyDetector\":\n",
    "            self.scaler = StandardScaler()\n",
    "            train_x = pd.DataFrame(self.scaler.fit_transform(train_x), columns=train_x.columns)\n",
    "            test_x = pd.DataFrame(self.scaler.transform(test_x), columns=test_x.columns)\n",
    "\n",
    "        self.feature_columns = train_x.columns.tolist()\n",
    "        logger.info(f\"Final feature preparation complete - Train: {train_x.shape}, Test: {test_x.shape}\")\n",
    "\n",
    "        return train_x, test_x, train_y, test_y\n",
    "\n",
    "    def _final_cleanup(self, train_x, test_x):\n",
    "        remaining_object_cols = train_x.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "        if remaining_object_cols:\n",
    "            logger.warning(f\"Dropping remaining object columns: {remaining_object_cols}\")\n",
    "            train_x = train_x.drop(columns=remaining_object_cols)\n",
    "            test_x = test_x.drop(columns=remaining_object_cols)\n",
    "\n",
    "        train_x = train_x.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "        test_x = test_x.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "        test_x = test_x.reindex(columns=train_x.columns, fill_value=0)\n",
    "        return train_x, test_x\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model_name == \"HeartRatePredictor\":\n",
    "            config = self.params.HEART_RATE_PREDICTOR\n",
    "            return RandomForestRegressor(\n",
    "                n_estimators=config.n_estimators,\n",
    "                max_depth=config.max_depth,\n",
    "                random_state=config.random_state\n",
    "            )\n",
    "        elif self.model_name == \"AnomalyDetector\":\n",
    "            config = self.params.ANOMALY_DETECTOR\n",
    "            return IsolationForest(\n",
    "                n_estimators=config.n_estimators,\n",
    "                max_samples=config.max_samples if hasattr(config, \"max_samples\") else \"auto\",\n",
    "                contamination=config.contamination,\n",
    "                random_state=config.random_state\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported model name: {self.model_name}\")\n",
    "\n",
    "    def _evaluate_regression(self, model, train_x, test_x, train_y, test_y):\n",
    "        train_preds = model.predict(train_x)\n",
    "        test_preds = model.predict(test_x)\n",
    "\n",
    "        metrics = {\n",
    "            \"train_rmse\": np.sqrt(mean_squared_error(train_y, train_preds)),\n",
    "            \"test_rmse\": np.sqrt(mean_squared_error(test_y, test_preds)),\n",
    "            \"train_mae\": np.mean(np.abs(train_y - train_preds)),\n",
    "            \"test_mae\": np.mean(np.abs(test_y - test_preds)),\n",
    "            \"test_r2\": r2_score(test_y, test_preds)\n",
    "        }\n",
    "\n",
    "        print(\"=== Regression Results ===\")\n",
    "        print(f\"Train RMSE: {metrics['train_rmse']:.4f}\")\n",
    "        print(f\"Test RMSE: {metrics['test_rmse']:.4f}\")\n",
    "        print(f\"Train MAE: {metrics['train_mae']:.4f}\")\n",
    "        print(f\"Test MAE: {metrics['test_mae']:.4f}\")\n",
    "        print(f\"Test R²: {metrics['test_r2']:.4f}\")\n",
    "\n",
    "        return metrics, test_preds\n",
    "\n",
    "    def _evaluate_classification(self, train_y, test_y, train_preds, test_preds):\n",
    "        train_y = np.where(train_y != 0, 1, 0)\n",
    "        test_y = np.where(test_y != 0, 1, 0)\n",
    "\n",
    "        metrics = {\n",
    "            \"train_accuracy\": accuracy_score(train_y, train_preds),\n",
    "            \"test_accuracy\": accuracy_score(test_y, test_preds),\n",
    "            \"test_precision\": precision_score(test_y, test_preds, zero_division=0),\n",
    "            \"test_recall\": recall_score(test_y, test_preds, zero_division=0),\n",
    "            \"test_f1\": f1_score(test_y, test_preds, zero_division=0)\n",
    "        }\n",
    "\n",
    "        print(\"=== Classification Results ===\")\n",
    "        print(f\"Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {metrics['test_precision']:.4f}\")\n",
    "        print(f\"Test Recall: {metrics['test_recall']:.4f}\")\n",
    "        print(f\"Test F1 Score: {metrics['test_f1']:.4f}\\n\")\n",
    "\n",
    "        print(\"=== Class Distribution ===\")\n",
    "        print(f\"Train - Normal: {(train_y == 0).sum()}, Anomaly: {(train_y == 1).sum()}\")\n",
    "        print(f\"Test - Normal: {(test_y == 0).sum()}, Anomaly: {(test_y == 1).sum()}\")\n",
    "        print(f\"Predictions - Normal: {(test_preds == 0).sum()}, Anomaly: {(test_preds == 1).sum()}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def save_model_artifacts(self, model, metrics=None, test_data=None):\n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "\n",
    "        model_artifacts = {\n",
    "            \"model\": model,\n",
    "            \"scaler\": self.scaler,\n",
    "            \"encoder\": self.encoder,\n",
    "            \"feature_columns\": self.feature_columns,\n",
    "            \"target_column\": self.target_column,\n",
    "            \"model_type\": type(model).__name__,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "\n",
    "        model_path = os.path.join(self.config.root_dir, f\"{self.model_name}.joblib\")\n",
    "        joblib.dump(model_artifacts, model_path)\n",
    "        logger.info(f\"Model artifacts saved at: {model_path}\")\n",
    "\n",
    "        if test_data is not None:\n",
    "            test_data_path = os.path.join(self.config.root_dir, f\"{self.model_name}_test_data.joblib\")\n",
    "            joblib.dump(test_data, test_data_path)\n",
    "            logger.info(f\"Test data saved at: {test_data_path}\")\n",
    "\n",
    "    def train_model(self):\n",
    "        try:\n",
    "            train_data, test_data = self.load_data()\n",
    "            train_x, test_x, train_y, test_y = self.prepare_features(train_data, test_data)\n",
    "\n",
    "            model = self.get_model()\n",
    "            logger.info(f\"Training {self.model_name} with features shape: {train_x.shape}\")\n",
    "\n",
    "            if self.model_name == \"HeartRatePredictor\":\n",
    "                model.fit(train_x, train_y)\n",
    "                metrics, predictions = self._evaluate_regression(model, train_x, test_x, train_y, test_y)\n",
    "\n",
    "            else:\n",
    "                model.fit(train_x)\n",
    "                train_preds = np.where(model.predict(train_x) == -1, 1, 0)\n",
    "                test_preds = np.where(model.predict(test_x) == -1, 1, 0)\n",
    "                metrics = self._evaluate_classification(train_y, test_y, train_preds, test_preds)\n",
    "                predictions = test_preds\n",
    "\n",
    "            test_data_combined = pd.concat([test_x, test_y], axis=1)\n",
    "            self.save_model_artifacts(model, metrics=metrics, test_data=test_data_combined)\n",
    "\n",
    "            logger.info(f\"Model training completed successfully for {self.model_name}\")\n",
    "            return model, predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model training: {str(e)}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694b2296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-06 02:23:58,392: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-06 02:23:58,398: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-06 02:23:58,405: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-09-06 02:23:58,406: INFO: common: created directory at: artifacts]\n",
      "[2025-09-06 02:23:58,407: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2025-09-06 02:24:23,286: INFO: 1128501030: Data loaded for HeartRatePredictor - Train: (226800, 336), Test: (75600, 336)]\n",
      "[2025-09-06 02:24:23,388: INFO: 1128501030: Dropping ID columns: ['user_id']]\n",
      "[2025-09-06 02:24:23,431: INFO: 1128501030: Processing categorical columns: ['gender', 'fitness_level', 'performance_level', 'activity_type', 'sleep_stage', 'date']]\n",
      "[2025-09-06 02:24:23,542: INFO: 1128501030: Column 'gender': 2 unique values]\n",
      "[2025-09-06 02:24:23,545: INFO: 1128501030: Column 'fitness_level': 3 unique values]\n",
      "[2025-09-06 02:24:23,546: INFO: 1128501030: Column 'performance_level': 3 unique values]\n",
      "[2025-09-06 02:24:23,547: INFO: 1128501030: Column 'activity_type': 6 unique values]\n",
      "[2025-09-06 02:24:23,548: INFO: 1128501030: Column 'sleep_stage': 3 unique values]\n",
      "[2025-09-06 02:24:23,549: INFO: 1128501030: Column 'date': 76104 unique values]\n",
      "[2025-09-06 02:24:23,550: INFO: 1128501030: Applying target encoding to: ['date']]\n",
      "[2025-09-06 02:24:23,948: INFO: 1128501030: Applying one-hot encoding to: ['gender', 'fitness_level', 'performance_level', 'activity_type', 'sleep_stage']]\n",
      "[2025-09-06 02:24:25,001: INFO: 1128501030: Final feature preparation complete - Train: (226800, 28), Test: (75600, 28)]\n",
      "[2025-09-06 02:24:25,003: INFO: 1128501030: Training HeartRatePredictor with features shape: (226800, 28)]\n",
      "=== Regression Results ===\n",
      "Train RMSE: 4.0151\n",
      "Test RMSE: 8.2909\n",
      "Train MAE: 2.9742\n",
      "Test MAE: 6.6035\n",
      "Test R²: 0.8896\n",
      "[2025-09-06 02:26:27,119: INFO: 1128501030: Model artifacts saved at: artifacts\\model_trainer\\HeartRatePredictor.joblib]\n",
      "[2025-09-06 02:26:27,138: INFO: 1128501030: Test data saved at: artifacts\\model_trainer\\HeartRatePredictor_test_data.joblib]\n",
      "[2025-09-06 02:26:27,140: INFO: 1128501030: Model training completed successfully for HeartRatePredictor]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samit\\AppData\\Local\\Temp\\ipykernel_22520\\1128501030.py:31: DtypeWarning: Columns (73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path)\n",
      "C:\\Users\\samit\\AppData\\Local\\Temp\\ipykernel_22520\\1128501030.py:32: DtypeWarning: Columns (73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_data = pd.read_csv(test_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-06 02:26:43,024: INFO: 1128501030: Data loaded for AnomalyDetector - Train: (226800, 336), Test: (75600, 336)]\n",
      "[2025-09-06 02:26:43,084: INFO: 1128501030: Processing categorical columns: ['activity_type', 'sleep_stage', 'date']]\n",
      "[2025-09-06 02:26:43,150: INFO: 1128501030: Column 'activity_type': 6 unique values]\n",
      "[2025-09-06 02:26:43,151: INFO: 1128501030: Column 'sleep_stage': 3 unique values]\n",
      "[2025-09-06 02:26:43,152: INFO: 1128501030: Column 'date': 76104 unique values]\n",
      "[2025-09-06 02:26:43,152: INFO: 1128501030: Applying target encoding to: ['date']]\n",
      "[2025-09-06 02:26:43,407: INFO: 1128501030: Applying one-hot encoding to: ['activity_type', 'sleep_stage']]\n",
      "[2025-09-06 02:26:43,966: INFO: 1128501030: Final feature preparation complete - Train: (226800, 20), Test: (75600, 20)]\n",
      "[2025-09-06 02:26:43,968: INFO: 1128501030: Training AnomalyDetector with features shape: (226800, 20)]\n",
      "=== Classification Results ===\n",
      "Train Accuracy: 0.9642\n",
      "Test Accuracy: 0.9489\n",
      "Test Precision: 0.0064\n",
      "Test Recall: 0.0467\n",
      "Test F1 Score: 0.0113\n",
      "\n",
      "=== Class Distribution ===\n",
      "Train - Normal: 225536, Anomaly: 1264\n",
      "Test - Normal: 75129, Anomaly: 471\n",
      "Predictions - Normal: 72162, Anomaly: 3438\n",
      "[2025-09-06 02:27:22,197: INFO: 1128501030: Model artifacts saved at: artifacts\\model_trainer\\AnomalyDetector.joblib]\n",
      "[2025-09-06 02:27:22,221: INFO: 1128501030: Test data saved at: artifacts\\model_trainer\\AnomalyDetector_test_data.joblib]\n",
      "[2025-09-06 02:27:22,224: INFO: 1128501030: Model training completed successfully for AnomalyDetector]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_trainer_config()\n",
    "    \n",
    "    hr_trainer = ModelTrainer(\n",
    "        config=model_trainer_config, \n",
    "        schema_config=config_manager.schema,  \n",
    "        params=config_manager.params,         \n",
    "        model_name=\"HeartRatePredictor\"\n",
    "    )\n",
    "    hr_model, hr_predictions = hr_trainer.train_model()   \n",
    "    \n",
    "    anomaly_trainer = ModelTrainer(\n",
    "        config=model_trainer_config, \n",
    "        schema_config=config_manager.schema,  \n",
    "        params=config_manager.params,         \n",
    "        model_name=\"AnomalyDetector\"\n",
    "    )\n",
    "    anomaly_model, anomaly_predictions = anomaly_trainer.train_model()\n",
    "         \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c04e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
