{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc526d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4d175e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2a27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b4aeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86d3ba0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a0117bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path_reading: Path\n",
    "    data_path_users: Path\n",
    "    target_column: str\n",
    "    features: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6961c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Heart_Rate_Anomaly_Detector.constants import *\n",
    "from Heart_Rate_Anomaly_Detector.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7279da81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self, model: str) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        \n",
    "        model_schema = self.schema.models.get(model)\n",
    "        if model_schema is None:\n",
    "            raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "        return DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path_reading=config.data_path.readings,\n",
    "            data_path_users=config.data_path.users,\n",
    "            target_column=model_schema.target_column,\n",
    "            features=model_schema.features\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4029c723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from Heart_Rate_Anomaly_Detector import logger\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b766b1e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scaler = None\n",
    "        self.imputers = {}\n",
    "        self.label_encoders = {}\n",
    "        \n",
    "    def load_and_preprocess_data(self):\n",
    "       \n",
    "        logger.info(\"Loading and preprocessing data...\")\n",
    "        \n",
    "        df1 = pd.read_csv(self.config.data_path_reading)\n",
    "        df2 = pd.read_csv(self.config.data_path_users)\n",
    "        \n",
    "        df = pd.merge(df1, df2, on='user_id')\n",
    "        logger.info(f\"Loaded data shape: {df.shape}\")\n",
    "        \n",
    "        df.rename(columns={'timestamp': 'date'}, inplace=True)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.sort_values('date')\n",
    "        \n",
    "        \n",
    "        df = df.infer_objects()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def handle_missing_values(self, df):\n",
    "        \n",
    "        df = df.copy()\n",
    "        logger.info(\"Handling missing values...\")      \n",
    "        missing_before = df.isnull().sum().sum()\n",
    "        logger.info(f\"Missing values before processing: {missing_before}\")\n",
    "        \n",
    "       \n",
    "        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "     \n",
    "        if 'date' in categorical_cols:\n",
    "            categorical_cols.remove('date')\n",
    "              \n",
    "        if numeric_cols:\n",
    "            df_with_date = df.set_index('date') if 'date' in df.columns else df\n",
    "            df_with_date[numeric_cols] = df_with_date[numeric_cols].interpolate(method='time')\n",
    "            df = df_with_date.reset_index() if 'date' in df_with_date.index.names else df_with_date\n",
    "            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())\n",
    "        \n",
    "        \n",
    "        if categorical_cols:\n",
    "            for col in categorical_cols:\n",
    "                df[col] = df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown')\n",
    "        \n",
    "        missing_after = df.isnull().sum().sum()\n",
    "        logger.info(f\"Missing values after processing: {missing_after}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_advanced_features(self, df):\n",
    "        \n",
    "        df = df.copy()\n",
    "        logger.info(\"Creating advanced features...\")\n",
    "        \n",
    "        \n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            \n",
    "            if 'month' not in df.columns:\n",
    "                df['month'] = df['date'].dt.month\n",
    "            if 'day' not in df.columns:\n",
    "                df['day'] = df['date'].dt.day\n",
    "            if 'year' not in df.columns:\n",
    "                df['year'] = df['date'].dt.year\n",
    "            if 'day_of_week' not in df.columns:\n",
    "                df['day_of_week'] = df['date'].dt.dayofweek\n",
    "            if 'hour' not in df.columns and df['date'].dt.hour.nunique() > 1:\n",
    "                df['hour'] = df['date'].dt.hour\n",
    "            if 'is_weekend' not in df.columns:\n",
    "                df['is_weekend'] = (df['date'].dt.dayofweek >= 5).astype(int)\n",
    "            if 'day_of_year' not in df.columns:\n",
    "                df['day_of_year'] = df['date'].dt.dayofyear\n",
    "            \n",
    "            \n",
    "            df['quarter'] = df['date'].dt.quarter\n",
    "            df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "            \n",
    "            \n",
    "            df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "            df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "            df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "            df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "            \n",
    "            if 'hour' in df.columns:\n",
    "                df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "                df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "            \n",
    "           \n",
    "            df['is_winter'] = df['month'].isin([12, 1, 2]).astype(int)\n",
    "            df['is_summer'] = df['month'].isin([6, 7, 8]).astype(int)\n",
    "            df['is_spring'] = df['month'].isin([3, 4, 5]).astype(int)\n",
    "            df['is_autumn'] = df['month'].isin([9, 10, 11]).astype(int)\n",
    "\n",
    "            if 'hrv_rmssd' in df.columns and 'stress_score' in df.columns:\n",
    "                df['hrv_stress_ratio'] = df['hrv_rmssd'] / (df['stress_score'] + 1e-8)\n",
    "                df['hrv_stress_interaction'] = df['hrv_rmssd'] * df['stress_score']\n",
    "            \n",
    "            \n",
    "            if 'sleep_stage' in df.columns:\n",
    "                df['is_deep_sleep'] = (df['sleep_stage'] == 'deep').astype(int)\n",
    "                df['is_rem_sleep'] = (df['sleep_stage'] == 'rem').astype(int)\n",
    "                df['is_light_sleep'] = (df['sleep_stage'] == 'light').astype(int)\n",
    "                df['is_awake'] = (df['sleep_stage'] == 'awake').astype(int)\n",
    "            \n",
    "        \n",
    "            if 'fitness_level' in df.columns:\n",
    "                fitness_map = {'beginner': 1, 'intermediate': 2, 'advanced': 3, 'elite': 4}\n",
    "                df['fitness_numeric'] = df['fitness_level'].str.lower().map(fitness_map).fillna(1)\n",
    "                \n",
    "                \n",
    "                if 'intensity_numeric' in df.columns:\n",
    "                    df['fitness_intensity_ratio'] = df['fitness_numeric'] / (df['intensity_numeric'] + 1e-8)\n",
    "            \n",
    "        \n",
    "            if 'performance_level' in df.columns:\n",
    "                perf_map = {'poor': 1, 'below_average': 2, 'average': 3, 'above_average': 4, 'excellent': 5}\n",
    "                df['performance_numeric'] = df['performance_level'].str.lower().map(perf_map).fillna(3)\n",
    "            \n",
    "            \n",
    "            if 'bmi' in df.columns:\n",
    "                df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce')\n",
    "                df['bmi_underweight'] = (df['bmi'] < 18.5).astype(int)\n",
    "                df['bmi_normal'] = ((df['bmi'] >= 18.5) & (df['bmi'] < 25)).astype(int)\n",
    "                df['bmi_overweight'] = ((df['bmi'] >= 25) & (df['bmi'] < 30)).astype(int)\n",
    "                df['bmi_obese'] = (df['bmi'] >= 30).astype(int)\n",
    "        \n",
    "            if 'signal_quality' in df.columns:\n",
    "                df['poor_signal'] = (df['signal_quality'] <= 0.5).astype(int)\n",
    "                df['excellent_signal'] = (df['signal_quality'] >= 0.9).astype(int)\n",
    "            \n",
    "            if 'device_battery' in df.columns:\n",
    "                df['low_battery'] = (df['device_battery'] <= 20).astype(int)\n",
    "            \n",
    "            \n",
    "            if 'is_anomaly' in df.columns:\n",
    "                df['anomaly_binary'] = df['is_anomaly'].astype(int)\n",
    "                \n",
    "            if 'anomaly_severity' in df.columns:\n",
    "                df['anomaly_severity'] = pd.to_numeric(df['anomaly_severity'], errors='coerce')\n",
    "                df['high_severity_anomaly'] = (df['anomaly_severity'] >= 0.8).astype(int)\n",
    "        \n",
    "        \n",
    "                \n",
    "        logger.info(f\"Features created. New shape: {df.shape}\")\n",
    "        return df\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "    def create_lag_features(self, df, target_col=None, lags=[1, 2, 5]):\n",
    "\n",
    "        df = df.copy()\n",
    "        target_col = target_col or getattr(self.config, 'target_column', 'heart_rate')\n",
    "\n",
    "        logger.info(f\"Creating Lag Features for target: {target_col} ...\")\n",
    "        df = df.sort_values(['user_id', 'date'])\n",
    "        \n",
    "        for lag in lags:\n",
    "            if target_col in df.columns:\n",
    "                df[f'{target_col}_lag_{lag}'] = df.groupby('user_id')[target_col].shift(lag)\n",
    "                df[f'{target_col}_diff_{lag}'] = df[target_col] - df[f'{target_col}_lag_{lag}']\n",
    "                df[target_col] = df[target_col].astype(float)\n",
    "                df[f'{target_col}_pct_change_{lag}'] = (df.groupby('user_id')[target_col].pct_change(lag).replace([np.inf, -np.inf], 0).fillna(0))\n",
    "\n",
    "\n",
    "\n",
    "            if 'hrv_rmssd' in df.columns:\n",
    "                df[f'hrv_rmssd_lag_{lag}'] = df.groupby('user_id')['hrv_rmssd'].shift(lag)\n",
    "            if 'stress_score' in df.columns:\n",
    "                df[f'stress_score_lag_{lag}'] = df.groupby('user_id')['stress_score'].shift(lag)\n",
    "            if 'steps_5min' in df.columns:\n",
    "                df[f'steps_5min_lag_{lag}'] = df.groupby('user_id')['steps_5min'].shift(lag)\n",
    "\n",
    "        logger.info(f\"Lag Features created. New shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    def create_rolling_features(self, df, target_col=None, windows=[5, 10, 30]):\n",
    "        df = df.copy()\n",
    "        target_col = target_col or getattr(self.config, 'target_column', 'heart_rate')\n",
    "\n",
    "        logger.info(f\"Creating Rolling Features for target: {target_col} ...\")\n",
    "        df = df.sort_values(['user_id', 'date'])\n",
    "\n",
    "        metrics = [target_col, 'hrv_rmssd', 'stress_score', 'steps_5min', 'calories_5min', 'skin_temperature']\n",
    "        metrics = [m for m in metrics if m in df.columns]\n",
    "\n",
    "        for metric in metrics:\n",
    "            for window in windows:\n",
    "                df[f'{metric}_rolling_mean_{window}'] = df.groupby('user_id')[metric].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "                )\n",
    "                df[f'{metric}_rolling_std_{window}'] = df.groupby('user_id')[metric].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "                )\n",
    "                df[f'{metric}_rolling_min_{window}'] = df.groupby('user_id')[metric].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "                )\n",
    "                df[f'{metric}_rolling_max_{window}'] = df.groupby('user_id')[metric].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "                )\n",
    "                df[f'{metric}_rolling_median_{window}'] = df.groupby('user_id')[metric].transform(\n",
    "                    lambda x: x.rolling(window=window, min_periods=1).median()\n",
    "                )\n",
    "\n",
    "                mean_vals = df[f'{metric}_rolling_mean_{window}']\n",
    "                median_vals = df[f'{metric}_rolling_median_{window}']\n",
    "                std_vals = df[f'{metric}_rolling_std_{window}']\n",
    "\n",
    "                df[f'{metric}_dist_from_mean_{window}'] = df[metric] - mean_vals\n",
    "                df[f'{metric}_dist_from_median_{window}'] = df[metric] - median_vals\n",
    "                df[f'{metric}_zscore_{window}'] = (df[metric] - mean_vals) / (std_vals + 1e-8)\n",
    "\n",
    "        logger.info(f\"Rolling Features created. New shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def create_user_specific_features(self, df, target_col=None):\n",
    "        df = df.copy()\n",
    "        target_col = target_col or getattr(self.config, 'target_column', 'heart_rate')\n",
    "\n",
    "        logger.info(f\"Creating User Specific Features for target: {target_col} ...\")\n",
    "\n",
    "        metrics_for_baseline = [target_col, 'hrv_rmssd', 'stress_score', 'steps_5min', 'calories_5min', 'skin_temperature']\n",
    "        metrics_for_baseline = [m for m in metrics_for_baseline if m in df.columns]\n",
    "        \n",
    "        for metric in metrics_for_baseline:\n",
    "            user_stats = df.groupby('user_id')[metric].agg([\n",
    "                'mean', 'std', 'min', 'max', 'median'\n",
    "            ]).reset_index()\n",
    "            user_stats.columns = ['user_id'] + [f'user_{metric}_{col}' for col in user_stats.columns[1:]]\n",
    "            \n",
    "            \n",
    "            df = df.merge(user_stats, on='user_id', how='left')\n",
    "            \n",
    "            \n",
    "            df[f'{metric}_above_baseline'] = df[metric] - df[f'user_{metric}_mean']\n",
    "            df[f'{metric}_zscore_user'] = df[f'{metric}_above_baseline'] / (df[f'user_{metric}_std'] + 1e-8)\n",
    "            \n",
    "            \n",
    "            df[f'{metric}_percentile_user'] = df.groupby('user_id')[metric].rank(pct=True)\n",
    "        \n",
    "      \n",
    "        if 'max_hr' in df.columns:\n",
    "            df['max_hr_to_use'] = df['max_hr'].fillna(220 - df['age'])\n",
    "        else:\n",
    "            df['max_hr_to_use'] = 220 - df['age']\n",
    "        \n",
    "        \n",
    "        df['hr_zone_1'] = (df[target_col] <= 0.6 * df['max_hr_to_use']).astype(int)  # Recovery\n",
    "        df['hr_zone_2'] = ((df[target_col] > 0.6 * df['max_hr_to_use']) & \n",
    "                          (df[target_col] <= 0.7 * df['max_hr_to_use'])).astype(int)  # Aerobic base\n",
    "        df['hr_zone_3'] = ((df[target_col] > 0.7 * df['max_hr_to_use']) & \n",
    "                          (df[target_col] <= 0.8 * df['max_hr_to_use'])).astype(int)  # Aerobic\n",
    "        df['hr_zone_4'] = ((df[target_col] > 0.8 * df['max_hr_to_use']) & \n",
    "                          (df[target_col] <= 0.9 * df['max_hr_to_use'])).astype(int)  # Threshold\n",
    "        df['hr_zone_5'] = (df[target_col] > 0.9 * df['max_hr_to_use']).astype(int)  # Anaerobic\n",
    "        \n",
    "     \n",
    "        if 'resting_hr' in df.columns:\n",
    "            df['hr_reserve'] = df['max_hr_to_use'] - df['resting_hr']\n",
    "            df['hr_reserve_pct'] = (df[target_col] - df['resting_hr']) / (df['hr_reserve'] + 1e-8)\n",
    "        \n",
    "        df['hr_pct_max'] = df[target_col] / df['max_hr_to_use']\n",
    "        \n",
    "        \n",
    "        if 'resting_hr_baseline' in df.columns:\n",
    "            df['hr_above_resting_baseline'] = df[target_col] - df['resting_hr_baseline']\n",
    "            df['hr_ratio_to_baseline'] = df[target_col] / (df['resting_hr_baseline'] + 1e-8)\n",
    "\n",
    "        logger.info(f\"User Specific Features created. New shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    def encode_medical_features(self, df):\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        logger.info(\"Creating Medical Features...\")\n",
    "        \n",
    "        \n",
    "        if 'conditions' in df.columns:\n",
    "            \n",
    "            df['conditions_clean'] = df['conditions'].fillna('none').str.lower()\n",
    "            \n",
    "            \n",
    "            df['has_diabetes'] = df['conditions_clean'].str.contains('diabetes', case=False, na=False).astype(int)\n",
    "            df['has_hypertension'] = df['conditions_clean'].str.contains('hypertension|high blood pressure', case=False, na=False).astype(int)\n",
    "            df['has_heart_disease'] = df['conditions_clean'].str.contains('heart disease|cardiac|coronary', case=False, na=False).astype(int)\n",
    "            df['has_asthma'] = df['conditions_clean'].str.contains('asthma', case=False, na=False).astype(int)\n",
    "            df['has_arrhythmia'] = df['conditions_clean'].str.contains('arrhythmia|irregular', case=False, na=False).astype(int)\n",
    "            df['has_thyroid'] = df['conditions_clean'].str.contains('thyroid|hyperthyroid|hypothyroid', case=False, na=False).astype(int)\n",
    "            \n",
    "           \n",
    "            condition_cols = ['has_diabetes', 'has_hypertension', 'has_heart_disease', \n",
    "                             'has_asthma', 'has_arrhythmia', 'has_thyroid']\n",
    "            df['num_medical_conditions'] = df[condition_cols].sum(axis=1)\n",
    "        \n",
    "       \n",
    "        if 'medications' in df.columns:\n",
    "            df['medications_clean'] = df['medications'].fillna('none').str.lower()\n",
    "            \n",
    "            \n",
    "            df['takes_beta_blockers'] = df['medications_clean'].str.contains(\n",
    "                'beta blocker|metoprolol|propranolol|atenolol|carvedilol', case=False, na=False\n",
    "            ).astype(int)\n",
    "            \n",
    "           \n",
    "            df['takes_ace_inhibitors'] = df['medications_clean'].str.contains(\n",
    "                'ace inhibitor|lisinopril|enalapril|captopril', case=False, na=False\n",
    "            ).astype(int)\n",
    "            \n",
    "            \n",
    "            df['takes_ccb'] = df['medications_clean'].str.contains(\n",
    "                'amlodipine|diltiazem|verapamil|nifedipine', case=False, na=False\n",
    "            ).astype(int)\n",
    "            \n",
    "           \n",
    "            df['takes_stimulants'] = df['medications_clean'].str.contains(\n",
    "                'adderall|ritalin|stimulant', case=False, na=False\n",
    "            ).astype(int)\n",
    "            \n",
    "            \n",
    "            med_cols = ['takes_beta_blockers', 'takes_ace_inhibitors', 'takes_ccb', 'takes_stimulants']\n",
    "            df['num_hr_affecting_medications'] = df[med_cols].sum(axis=1)\n",
    "        \n",
    "        \n",
    "        lifestyle_factors = ['smoker', 'caffeine_user', 'alcohol_user']\n",
    "        for factor in lifestyle_factors:\n",
    "            if factor in df.columns:\n",
    "                df[f'{factor}_binary'] = (df[factor] == True).astype(int)\n",
    "        \n",
    "        \n",
    "        if 'sleep_quality' in df.columns:\n",
    "            df['sleep_quality'] = pd.to_numeric(df['sleep_quality'], errors='coerce')\n",
    "            df['poor_sleep'] = (df['sleep_quality'] <= 2).astype(int)\n",
    "            df['excellent_sleep'] = (df['sleep_quality'] >= 4).astype(int)\n",
    "        \n",
    "        logger.info(f\"Medical Features created. New shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def encode_activity_features(self, df):\n",
    "\n",
    "        df = df.copy()\n",
    "\n",
    "        logger.info(\"Creating Activity Features...\")\n",
    "        \n",
    "        \n",
    "        if 'activity_type' in df.columns:\n",
    "            \n",
    "            df['activity_type_clean'] = df['activity_type'].fillna('unknown').astype(str).str.lower()\n",
    "\n",
    "            \n",
    "            if 'activity_type' not in self.label_encoders:\n",
    "                self.label_encoders['activity_type'] = LabelEncoder()\n",
    "                df['activity_type_encoded'] = self.label_encoders['activity_type'].fit_transform(\n",
    "                    df['activity_type_clean']\n",
    "                )\n",
    "            else:\n",
    "                df['activity_type_encoded'] = self.label_encoders['activity_type'].transform(\n",
    "                    df['activity_type_clean']\n",
    "                )\n",
    "            \n",
    "            \n",
    "            df['is_running'] = df['activity_type_clean'].str.contains('run|jog', case=False, na=False).astype(int)\n",
    "            df['is_walking'] = df['activity_type_clean'].str.contains('walk', case=False, na=False).astype(int)\n",
    "            df['is_cycling'] = df['activity_type_clean'].str.contains('cycl|bike', case=False, na=False).astype(int)\n",
    "            df['is_strength'] = df['activity_type_clean'].str.contains('strength|weight|lift|gym', case=False, na=False).astype(int)\n",
    "            df['is_cardio'] = df['activity_type_clean'].str.contains('cardio|aerobic', case=False, na=False).astype(int)\n",
    "            df['is_resting'] = df['activity_type_clean'].str.contains('rest|sleep|sitting', case=False, na=False).astype(int)\n",
    "            df['is_swimming'] = df['activity_type_clean'].str.contains('swim', case=False, na=False).astype(int)\n",
    "            df['is_yoga'] = df['activity_type_clean'].str.contains('yoga|pilates', case=False, na=False).astype(int)\n",
    "        \n",
    "        \n",
    "        if 'activity_intensity' in df.columns:\n",
    "            \n",
    "            df['activity_intensity_clean'] = df['activity_intensity'].fillna('unknown').astype(str).str.lower()\n",
    "           \n",
    "            \n",
    "            intensity_map = {\n",
    "                'rest': 0, 'resting': 0,\n",
    "                'low': 1, 'light': 1, 'easy': 1,\n",
    "                'moderate': 2, 'medium': 2,\n",
    "                'high': 3, 'vigorous': 3, 'hard': 3,\n",
    "                'very_high': 4, 'maximum': 4, 'max': 4, 'very high': 4\n",
    "            }\n",
    "            \n",
    "            df['intensity_numeric'] = df['activity_intensity_clean'].map(intensity_map).fillna(0)\n",
    "            \n",
    "         \n",
    "            df['is_rest'] = (df['intensity_numeric'] == 0).astype(int)\n",
    "            df['is_low_intensity'] = (df['intensity_numeric'] == 1).astype(int)\n",
    "            df['is_moderate_intensity'] = (df['intensity_numeric'] == 2).astype(int)\n",
    "            df['is_high_intensity'] = (df['intensity_numeric'] >= 3).astype(int)\n",
    "        \n",
    "        \n",
    "        if 'steps_5min' in df.columns:\n",
    "            df['steps_5min_log'] = np.log1p(df['steps_5min'].fillna(0))\n",
    "            df['is_high_steps'] = (df['steps_5min'] > df['steps_5min'].quantile(0.8)).astype(int)\n",
    "            df['is_sedentary'] = (df['steps_5min'] <= 10).astype(int)\n",
    "        \n",
    "        if 'calories_5min' in df.columns:\n",
    "            df['calories_5min_log'] = np.log1p(df['calories_5min'].fillna(0))\n",
    "            df['is_high_calorie_burn'] = (df['calories_5min'] > df['calories_5min'].quantile(0.8)).astype(int)\n",
    "        \n",
    "        # Elevation impact\n",
    "        if 'elevation_gain' in df.columns:\n",
    "            df['elevation_gain_log'] = np.log1p(df['elevation_gain'].fillna(0) + 1)\n",
    "            df['is_high_elevation'] = (df['elevation_gain'] > 100).astype(int)\n",
    "        \n",
    "        logger.info(f\"Activity Features created. New shape: {df.shape}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "        \n",
    "    def train_test_splitting(self):\n",
    "        logger.info(\"Starting comprehensive data transformation pipeline...\")\n",
    "\n",
    "        df = self.load_and_preprocess_data()\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.create_advanced_features(df)\n",
    "\n",
    "        target_col = getattr(self.config, 'target_column', 'heart_rate')\n",
    "        df = self.create_lag_features(df, target_col=target_col)\n",
    "        df = self.create_rolling_features(df, target_col=target_col)\n",
    "        df = self.create_user_specific_features(df, target_col=target_col)\n",
    "        df = self.encode_medical_features(df)\n",
    "        df = self.encode_activity_features(df)\n",
    "\n",
    "        \n",
    "        \n",
    "        logger.info(\"Performing train-test split...\")\n",
    "        \n",
    "       \n",
    "        df = df.sort_values('date') if 'date' in df.columns else df\n",
    "        \n",
    "        \n",
    "        split_ratio = getattr(self.config, 'train_split_ratio', 0.75)\n",
    "        split_index = int(len(df) * split_ratio)\n",
    "        \n",
    "        train = df.iloc[:split_index].copy()\n",
    "        test = df.iloc[split_index:].copy()\n",
    "        \n",
    "        \n",
    "        if 'date' in train.columns:\n",
    "            train = train.reset_index(drop=True)\n",
    "            test = test.reset_index(drop=True)\n",
    "        \n",
    "       \n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        train.to_csv(os.path.join(self.config.root_dir, f\"train_{target_col}.csv\"), index=False)\n",
    "        test.to_csv(os.path.join(self.config.root_dir, f\"test_{target_col}.csv\"), index=False)\n",
    "\n",
    "        \n",
    "        \n",
    "        if self.scaler is not None:\n",
    "            scaler_path = os.path.join(self.config.root_dir, \"scaler.joblib\")\n",
    "            joblib.dump(self.scaler, scaler_path)\n",
    "            logger.info(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "        \n",
    "        if self.label_encoders:\n",
    "            encoders_path = os.path.join(self.config.root_dir, \"label_encoders.joblib\")\n",
    "            joblib.dump(self.label_encoders, encoders_path)\n",
    "            logger.info(f\"Label encoders saved to: {encoders_path}\")\n",
    "\n",
    "        \n",
    "        \n",
    "   \n",
    "           \n",
    "        logger.info(\"Data transformation pipeline completed successfully!\")\n",
    "        \n",
    "       \n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f058891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-01 23:04:03,704: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-01 23:04:03,706: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-01 23:04:03,714: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-09-01 23:04:03,717: INFO: common: created directory at: artifacts]\n",
      "[2025-09-01 23:04:03,718: INFO: 3230879530: Starting comprehensive data transformation pipeline...]\n",
      "[2025-09-01 23:04:03,720: INFO: 3230879530: Loading and preprocessing data...]\n",
      "[2025-09-01 23:04:04,664: INFO: 3230879530: Loaded data shape: (302400, 37)]\n",
      "[2025-09-01 23:04:05,104: INFO: 3230879530: Handling missing values...]\n",
      "[2025-09-01 23:04:05,282: INFO: 3230879530: Missing values before processing: 801563]\n",
      "[2025-09-01 23:04:06,683: INFO: 3230879530: Missing values after processing: 0]\n",
      "[2025-09-01 23:04:06,867: INFO: 3230879530: Creating advanced features...]\n",
      "[2025-09-01 23:04:07,613: INFO: 3230879530: Features created. New shape: (302400, 73)]\n",
      "[2025-09-01 23:04:07,876: INFO: 3230879530: Creating Lag Features for target: heart_rate ...]\n",
      "[2025-09-01 23:04:08,552: INFO: 3230879530: Lag Features created. New shape: (302400, 91)]\n",
      "[2025-09-01 23:04:08,720: INFO: 3230879530: Creating Rolling Features for target: heart_rate ...]\n",
      "[2025-09-01 23:04:15,712: INFO: 3230879530: Rolling Features created. New shape: (302400, 235)]\n",
      "[2025-09-01 23:04:16,323: INFO: 3230879530: Creating User Specific Features for target: heart_rate ...]\n",
      "[2025-09-01 23:04:21,229: INFO: 3230879530: User Specific Features created. New shape: (302400, 294)]\n",
      "[2025-09-01 23:04:22,043: INFO: 3230879530: Creating Medical Features...]\n",
      "[2025-09-01 23:04:23,889: INFO: 3230879530: Medical Features created. New shape: (302400, 313)]\n",
      "[2025-09-01 23:04:24,875: INFO: 3230879530: Creating Activity Features...]\n",
      "[2025-09-01 23:04:26,519: INFO: 3230879530: Activity Features created. New shape: (302400, 336)]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 302400 entries, 0 to 302399\n",
      "Columns: 336 entries, date to is_high_elevation\n",
      "dtypes: UInt32(1), bool(4), datetime64[ns](1), float64(236), int32(7), int64(71), object(16)\n",
      "memory usage: 758.2+ MB\n",
      "None\n",
      "[2025-09-01 23:04:26,635: INFO: 3230879530: Performing train-test split...]\n",
      "[2025-09-01 23:06:22,801: INFO: 3230879530: Label encoders saved to: artifacts/data_transformation\\label_encoders.joblib]\n",
      "[2025-09-01 23:06:22,806: INFO: 3230879530: Data transformation pipeline completed successfully!]\n",
      "[2025-09-01 23:06:23,161: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-01 23:06:23,179: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-01 23:06:23,217: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-09-01 23:06:23,224: INFO: common: created directory at: artifacts]\n",
      "[2025-09-01 23:06:23,227: INFO: 3230879530: Starting comprehensive data transformation pipeline...]\n",
      "[2025-09-01 23:06:23,230: INFO: 3230879530: Loading and preprocessing data...]\n",
      "[2025-09-01 23:06:28,653: INFO: 3230879530: Loaded data shape: (302400, 37)]\n",
      "[2025-09-01 23:06:30,816: INFO: 3230879530: Handling missing values...]\n",
      "[2025-09-01 23:06:31,834: INFO: 3230879530: Missing values before processing: 801563]\n",
      "[2025-09-01 23:06:38,467: INFO: 3230879530: Missing values after processing: 0]\n",
      "[2025-09-01 23:06:38,768: INFO: 3230879530: Creating advanced features...]\n",
      "[2025-09-01 23:06:40,025: INFO: 3230879530: Features created. New shape: (302400, 73)]\n",
      "[2025-09-01 23:06:40,320: INFO: 3230879530: Creating Lag Features for target: is_anomaly ...]\n",
      "[2025-09-01 23:06:41,163: INFO: 3230879530: Lag Features created. New shape: (302400, 91)]\n",
      "[2025-09-01 23:06:41,414: INFO: 3230879530: Creating Rolling Features for target: is_anomaly ...]\n",
      "[2025-09-01 23:06:51,085: INFO: 3230879530: Rolling Features created. New shape: (302400, 235)]\n",
      "[2025-09-01 23:06:51,838: INFO: 3230879530: Creating User Specific Features for target: is_anomaly ...]\n",
      "[2025-09-01 23:06:58,427: INFO: 3230879530: User Specific Features created. New shape: (302400, 294)]\n",
      "[2025-09-01 23:06:59,356: INFO: 3230879530: Creating Medical Features...]\n",
      "[2025-09-01 23:07:01,571: INFO: 3230879530: Medical Features created. New shape: (302400, 313)]\n",
      "[2025-09-01 23:07:02,749: INFO: 3230879530: Creating Activity Features...]\n",
      "[2025-09-01 23:07:04,888: INFO: 3230879530: Activity Features created. New shape: (302400, 336)]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 302400 entries, 0 to 302399\n",
      "Columns: 336 entries, date to is_high_elevation\n",
      "dtypes: UInt32(1), bool(3), datetime64[ns](1), float64(235), int32(7), int64(71), object(18)\n",
      "memory usage: 760.2+ MB\n",
      "None\n",
      "[2025-09-01 23:07:05,002: INFO: 3230879530: Performing train-test split...]\n",
      "[2025-09-01 23:08:44,563: INFO: 3230879530: Label encoders saved to: artifacts/data_transformation\\label_encoders.joblib]\n",
      "[2025-09-01 23:08:44,564: INFO: 3230879530: Data transformation pipeline completed successfully!]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    heart_rate_config = ConfigurationManager().get_data_transformation_config(model=\"HeartRatePredictor\")\n",
    "    transformer = DataTransformation(heart_rate_config)\n",
    "    train_hr, test_hr = transformer.train_test_splitting()\n",
    "\n",
    "    \n",
    "    anomaly_config = ConfigurationManager().get_data_transformation_config(model=\"AnomalyDetector\")\n",
    "    transformer = DataTransformation(anomaly_config)\n",
    "    train_anom, test_anom = transformer.train_test_splitting()\n",
    " \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac772d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f476e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
