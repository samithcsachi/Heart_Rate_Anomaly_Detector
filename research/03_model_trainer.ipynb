{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffbfdd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd283915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9827ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35fde99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\SAMITH\\\\Github\\\\Heart_Rate_Anomaly_Detector'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a90442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63aad981",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_heart_rate_data_path: Path\n",
    "    test_heart_rate_data_path: Path\n",
    "    train_is_anomaly_data_path: Path\n",
    "    test_is_anomaly_data_path: Path\n",
    "    data_transformation_dir: Path  \n",
    "    \n",
    "   \n",
    "    heart_rate_predictor_model_name: str\n",
    "    anomaly_detector_model_name: str\n",
    "    \n",
    "    \n",
    "    heart_rate_target_column: str\n",
    "    anomaly_target_column: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876ada10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Heart_Rate_Anomaly_Detector.constants import *\n",
    "from Heart_Rate_Anomaly_Detector.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4672ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "        \n",
    "        \n",
    "        heart_rate_params = self.params.HEART_RATE_PREDICTOR\n",
    "        anomaly_params = self.params.ANOMALY_DETECTOR\n",
    "        \n",
    "       \n",
    "        heart_rate_model_schema = self.schema.models.HeartRatePredictor\n",
    "        anomaly_model_schema = self.schema.models.AnomalyDetector\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            train_heart_rate_data_path=Path(config.data_path.train_heart_rate),\n",
    "            test_heart_rate_data_path=Path(config.data_path.test_heart_rate),\n",
    "            train_is_anomaly_data_path=Path(config.data_path.train_is_anomaly),\n",
    "            test_is_anomaly_data_path=Path(config.data_path.test_is_anomaly),\n",
    "            data_transformation_dir=Path(self.config.data_transformation.root_dir),\n",
    "            \n",
    "         \n",
    "            heart_rate_predictor_model_name=config.model_name.heart_rate_predictor,\n",
    "            anomaly_detector_model_name=config.model_name.anomaly_detector,\n",
    "            \n",
    "           \n",
    "            heart_rate_target_column=heart_rate_model_schema.target_column,\n",
    "            anomaly_target_column=anomaly_model_schema.target_column\n",
    "        )\n",
    "        return model_trainer_config\n",
    "    \n",
    "    def get_heart_rate_features(self) -> list:\n",
    "       \n",
    "        return self.schema.models.HeartRatePredictor.features\n",
    "    \n",
    "    def get_anomaly_features(self) -> list:\n",
    "       \n",
    "        return self.schema.models.AnomalyDetector.features\n",
    "    \n",
    "    def get_column_dtypes(self) -> dict:\n",
    "        \n",
    "        return self.schema.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46fc7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from Heart_Rate_Anomaly_Detector import logger\n",
    "from sklearn.ensemble import RandomForestRegressor, IsolationForest\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9553cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config, schema_config, params, model_name):\n",
    "        self.config = config\n",
    "        self.schema_config = schema_config\n",
    "        self.params = params\n",
    "        self.model_name = model_name\n",
    "        self.scaler = None\n",
    "        self.feature_columns = None\n",
    "        self.encoder = None  \n",
    "        \n",
    "        self.target_column = schema_config.models[model_name].target_column\n",
    "        self.features = schema_config.models[model_name].features\n",
    "\n",
    "    def load_data(self):\n",
    "      \n",
    "        data_paths = {\n",
    "            \"HeartRatePredictor\": (self.config.train_heart_rate_data_path, self.config.test_heart_rate_data_path),\n",
    "            \"AnomalyDetector\": (self.config.train_is_anomaly_data_path, self.config.test_is_anomaly_data_path)\n",
    "        }\n",
    "        \n",
    "        if self.model_name not in data_paths:\n",
    "            raise ValueError(f\"Unknown model name: {self.model_name}\")\n",
    "        \n",
    "        train_path, test_path = data_paths[self.model_name]\n",
    "        train_data = pd.read_csv(train_path)\n",
    "        test_data = pd.read_csv(test_path)\n",
    "        \n",
    "        logger.info(f\"Data loaded for {self.model_name} - Train: {train_data.shape}, Test: {test_data.shape}\")\n",
    "        return train_data, test_data\n",
    "\n",
    "    def _encode_categorical_features(self, train_x, test_x, train_y):\n",
    "        \n",
    "        cat_cols = train_x.select_dtypes(include=['object']).columns.tolist()\n",
    "        \n",
    "      \n",
    "        exclude_cols = ['user_id', 'email', 'name']\n",
    "        id_cols_to_drop = [col for col in exclude_cols if col in train_x.columns]\n",
    "        if id_cols_to_drop:\n",
    "            logger.info(f\"Dropping ID columns: {id_cols_to_drop}\")\n",
    "            train_x = train_x.drop(columns=id_cols_to_drop)\n",
    "            test_x = test_x.drop(columns=id_cols_to_drop)\n",
    "            cat_cols = [col for col in cat_cols if col not in id_cols_to_drop]\n",
    "        \n",
    "        if not cat_cols:\n",
    "            return train_x, test_x\n",
    "            \n",
    "        logger.info(f\"Processing categorical columns: {cat_cols}\")\n",
    "        \n",
    "        \n",
    "        cardinality_info = {}\n",
    "        for col in cat_cols:\n",
    "            unique_count = train_x[col].nunique()\n",
    "            cardinality_info[col] = unique_count\n",
    "            logger.info(f\"Column '{col}': {unique_count} unique values\")\n",
    "        \n",
    "       \n",
    "        high_card_cols = [col for col, count in cardinality_info.items() if count > 20]\n",
    "        low_card_cols = [col for col, count in cardinality_info.items() if count <= 20]\n",
    "        \n",
    "        \n",
    "        if high_card_cols:\n",
    "            train_x, test_x = self._apply_target_encoding(train_x, test_x, train_y, high_card_cols)\n",
    "        \n",
    "       \n",
    "        if low_card_cols:\n",
    "            train_x, test_x = self._apply_onehot_encoding(train_x, test_x, low_card_cols)\n",
    "            \n",
    "        return train_x, test_x\n",
    "\n",
    "    def _apply_target_encoding(self, train_x, test_x, train_y, cols):\n",
    "     \n",
    "        logger.info(f\"Applying target encoding to: {cols}\")\n",
    "        \n",
    "        for col in cols:\n",
    "            target_mean = train_y.mean()\n",
    "            \n",
    "           \n",
    "            temp_df = pd.DataFrame({\n",
    "                'category': train_x[col],\n",
    "                'target': train_y.values\n",
    "            })\n",
    "            encoding_map = temp_df.groupby('category')['target'].mean().to_dict()\n",
    "            \n",
    "            train_x[f'{col}_encoded'] = train_x[col].map(encoding_map).fillna(target_mean)\n",
    "            test_x[f'{col}_encoded'] = test_x[col].map(encoding_map).fillna(target_mean)\n",
    "            \n",
    "            train_x = train_x.drop(columns=[col])\n",
    "            test_x = test_x.drop(columns=[col])\n",
    "            \n",
    "        return train_x, test_x\n",
    "\n",
    "    def _apply_onehot_encoding(self, train_x, test_x, cols):\n",
    "        \n",
    "        logger.info(f\"Applying one-hot encoding to: {cols}\")\n",
    "        \n",
    "      \n",
    "        expected_features = sum(train_x[col].nunique() - 1 for col in cols)\n",
    "        if expected_features > 500:\n",
    "            logger.warning(f\"Too many expected features ({expected_features}), switching to frequency encoding\")\n",
    "            return self._apply_frequency_encoding(train_x, test_x, cols)\n",
    "        \n",
    "        try:\n",
    "            self.encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "            train_encoded = self.encoder.fit_transform(train_x[cols])\n",
    "            test_encoded = self.encoder.transform(test_x[cols])\n",
    "            \n",
    "            feature_names = self.encoder.get_feature_names_out(cols)\n",
    "            train_encoded_df = pd.DataFrame(train_encoded, columns=feature_names, index=train_x.index)\n",
    "            test_encoded_df = pd.DataFrame(test_encoded, columns=feature_names, index=test_x.index)\n",
    "            \n",
    "           \n",
    "            remaining_cols = [col for col in train_x.columns if col not in cols]\n",
    "            train_x = pd.concat([train_x[remaining_cols], train_encoded_df], axis=1)\n",
    "            test_x = pd.concat([test_x[remaining_cols], test_encoded_df], axis=1)\n",
    "            \n",
    "        except MemoryError:\n",
    "            logger.error(\"Memory error in one-hot encoding, falling back to frequency encoding\")\n",
    "            return self._apply_frequency_encoding(train_x, test_x, cols)\n",
    "            \n",
    "        return train_x, test_x\n",
    "\n",
    "    def _apply_frequency_encoding(self, train_x, test_x, cols):\n",
    "      \n",
    "        logger.info(f\"Applying frequency encoding to: {cols}\")\n",
    "        \n",
    "        for col in cols:\n",
    "            freq_map = train_x[col].value_counts(normalize=True).to_dict()\n",
    "            train_x[f'{col}_freq'] = train_x[col].map(freq_map)\n",
    "            test_x[f'{col}_freq'] = test_x[col].map(freq_map).fillna(0)\n",
    "            \n",
    "            train_x = train_x.drop(columns=[col])\n",
    "            test_x = test_x.drop(columns=[col])\n",
    "            \n",
    "        return train_x, test_x\n",
    "\n",
    "    def prepare_features(self, train_data, test_data):\n",
    "        available_features = [col for col in self.features if col in train_data.columns]\n",
    "        \n",
    "        train_x = train_data[available_features].copy()\n",
    "        test_x = test_data[available_features].copy()\n",
    "        \n",
    "        train_y = train_data[self.target_column]\n",
    "        test_y = test_data[self.target_column]\n",
    "        \n",
    "       \n",
    "        train_x, test_x = self._encode_categorical_features(train_x, test_x, train_y)\n",
    "        \n",
    "       \n",
    "        train_x, test_x = self._final_cleanup(train_x, test_x)\n",
    "        \n",
    "        \n",
    "        if self.model_name == \"AnomalyDetector\":\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            self.scaler = StandardScaler()\n",
    "            train_x = pd.DataFrame(self.scaler.fit_transform(train_x), columns=train_x.columns)\n",
    "            test_x = pd.DataFrame(self.scaler.transform(test_x), columns=test_x.columns)\n",
    "        \n",
    "        self.feature_columns = train_x.columns.tolist()\n",
    "        logger.info(f\"Final feature preparation complete - Train: {train_x.shape}, Test: {test_x.shape}\")\n",
    "        \n",
    "        return train_x, test_x, train_y, test_y\n",
    "    \n",
    "    def _final_cleanup(self, train_x, test_x):\n",
    "        \n",
    "        remaining_object_cols = train_x.select_dtypes(include=['object']).columns.tolist()\n",
    "        if remaining_object_cols:\n",
    "            logger.warning(f\"Dropping remaining object columns: {remaining_object_cols}\")\n",
    "            train_x = train_x.drop(columns=remaining_object_cols)\n",
    "            test_x = test_x.drop(columns=remaining_object_cols)\n",
    "        \n",
    "        \n",
    "        train_x = train_x.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        test_x = test_x.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        \n",
    "        test_x = test_x.reindex(columns=train_x.columns, fill_value=0)\n",
    "        \n",
    "        return train_x, test_x\n",
    "\n",
    "    def get_model(self):\n",
    "        if self.model_name == \"HeartRatePredictor\":\n",
    "            config = self.params.HEART_RATE_PREDICTOR\n",
    "            return RandomForestRegressor(\n",
    "                n_estimators=config.n_estimators,\n",
    "                max_depth=config.max_depth,\n",
    "                random_state=config.random_state\n",
    "            )\n",
    "        elif self.model_name == \"AnomalyDetector\":\n",
    "            config = self.params.ANOMALY_DETECTOR\n",
    "            \n",
    "            if config.algorithm.lower() == \"lightgbm\":\n",
    "                \n",
    "                model = LGBMClassifier(\n",
    "                    n_estimators=config.n_estimators,\n",
    "                    learning_rate=config.learning_rate,\n",
    "                    max_depth=config.max_depth,\n",
    "                    random_state=config.random_state\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported anomaly detector: {config.algorithm}\")\n",
    "\n",
    "            return model\n",
    "\n",
    "    def _evaluate_regression(self, model, train_x, test_x, train_y, test_y):\n",
    "       \n",
    "        train_preds = model.predict(train_x)\n",
    "        test_preds = model.predict(test_x)\n",
    "        \n",
    "        train_rmse = np.sqrt(mean_squared_error(train_y, train_preds))\n",
    "        test_rmse = np.sqrt(mean_squared_error(test_y, test_preds))\n",
    "        test_r2 = r2_score(test_y, test_preds)\n",
    "        \n",
    "       \n",
    "        train_mae = np.mean(np.abs(train_y - train_preds))\n",
    "        test_mae = np.mean(np.abs(test_y - test_preds))\n",
    "        \n",
    "        metrics = {\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"test_rmse\": test_rmse,\n",
    "            \"train_mae\": train_mae,\n",
    "            \"test_mae\": test_mae,\n",
    "            \"test_r2\": test_r2\n",
    "        }\n",
    "        \n",
    "        print(\"=== Regression Results ===\")\n",
    "        print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "        print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "        print(f\"Train MAE: {train_mae:.4f}\")\n",
    "        print(f\"Test MAE: {test_mae:.4f}\")\n",
    "        print(f\"Test R²: {test_r2:.4f}\")\n",
    "        \n",
    "        return metrics, test_preds\n",
    "\n",
    "    def _evaluate_classification(self, model, train_x, test_x, train_y, test_y):\n",
    "       \n",
    "        train_preds = model.predict(train_x)\n",
    "        test_preds = model.predict(test_x)\n",
    "        \n",
    "        metrics = {\n",
    "            \"train_accuracy\": accuracy_score(train_y, train_preds),\n",
    "            \"test_accuracy\": accuracy_score(test_y, test_preds),\n",
    "            \"test_precision\": precision_score(test_y, test_preds, zero_division=0),\n",
    "            \"test_recall\": recall_score(test_y, test_preds, zero_division=0),\n",
    "            \"test_f1\": f1_score(test_y, test_preds, zero_division=0)\n",
    "        }\n",
    "        \n",
    "      \n",
    "        print(\"=== Classification Results ===\")\n",
    "        print(f\"Train Accuracy: {metrics['train_accuracy']:.4f}\")\n",
    "        print(f\"Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"Test Precision: {metrics['test_precision']:.4f}\")\n",
    "        print(f\"Test Recall: {metrics['test_recall']:.4f}\")\n",
    "        print(f\"Test F1 Score: {metrics['test_f1']:.4f}\\n\")\n",
    "        \n",
    "        print(\"=== Class Distribution ===\")\n",
    "        print(f\"Train - Normal: {(train_y == 0).sum()}, Anomaly: {(train_y == 1).sum()}\")\n",
    "        print(f\"Test - Normal: {(test_y == 0).sum()}, Anomaly: {(test_y == 1).sum()}\")\n",
    "        print(f\"Predictions - Normal: {(test_preds == 0).sum()}, Anomaly: {(test_preds == 1).sum()}\")\n",
    "        \n",
    "        return metrics, test_preds\n",
    "        \n",
    "\n",
    "    def save_model_artifacts(self, model, metrics=None, test_data=None):\n",
    "        \n",
    "        os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "        \n",
    "        model_artifacts = {\n",
    "            'model': model,\n",
    "            'scaler': self.scaler,\n",
    "            'encoder': self.encoder,\n",
    "            'feature_columns': self.feature_columns,\n",
    "            'target_column': self.target_column,\n",
    "            'model_type': type(model).__name__,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'metrics': metrics\n",
    "        }\n",
    "        \n",
    "        model_path = os.path.join(self.config.root_dir, f\"{self.model_name}.joblib\")\n",
    "        joblib.dump(model_artifacts, model_path)\n",
    "        logger.info(f\"Model artifacts saved at: {model_path}\")\n",
    "        \n",
    "        if test_data is not None:\n",
    "            test_data_path = os.path.join(self.config.root_dir, f\"{self.model_name}_test_data.joblib\")\n",
    "            joblib.dump(test_data, test_data_path)\n",
    "            logger.info(f\"Test data saved at: {test_data_path}\")\n",
    "        \n",
    "      \n",
    "        if hasattr(model, 'feature_importances_') and self.feature_columns:\n",
    "            self._save_feature_importance(model)\n",
    "\n",
    "    def _save_feature_importance(self, model):\n",
    "        \n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': self.feature_columns,\n",
    "            'importance': model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        importance_path = os.path.join(self.config.root_dir, f\"{self.model_name}_feature_importance.csv\")\n",
    "        feature_importance.to_csv(importance_path, index=False)\n",
    "        logger.info(f\"Feature importance saved at: {importance_path}\")\n",
    "        \n",
    "        logger.info(\"=== Top 10 Important Features ===\")\n",
    "        for idx, row in feature_importance.head(10).iterrows():\n",
    "            logger.info(f\"{row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "    def train_model(self):\n",
    "        try:\n",
    "            train_data, test_data = self.load_data()\n",
    "            train_x, test_x, train_y, test_y = self.prepare_features(train_data, test_data)\n",
    "                            \n",
    "            model = self.get_model()\n",
    "            logger.info(f\"Training {self.model_name} with features shape: {train_x.shape}\")\n",
    "                        \n",
    "            model.fit(train_x, train_y)\n",
    "                          \n",
    "            \n",
    "            if self.model_name == \"HeartRatePredictor\":\n",
    "                metrics, predictions = self._evaluate_regression(model, train_x, test_x, train_y, test_y)\n",
    "            else:  \n",
    "                metrics, predictions = self._evaluate_classification(model, train_x, test_x, train_y, test_y)\n",
    "                      \n",
    "            test_data_combined = pd.concat([test_x, test_y], axis=1)\n",
    "            self.save_model_artifacts(model, metrics=metrics, test_data=test_data_combined)\n",
    "            \n",
    "            logger.info(f\"Model training completed successfully for {self.model_name}\")\n",
    "            return model, predictions\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in model training: {str(e)}\")\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "694b2296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 00:36:14,062: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-09-05 00:36:14,068: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-09-05 00:36:14,076: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-09-05 00:36:14,077: INFO: common: created directory at: artifacts]\n",
      "[2025-09-05 00:36:14,078: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2025-09-05 00:36:34,958: INFO: 4093871268: Data loaded for HeartRatePredictor - Train: (226800, 336), Test: (75600, 336)]\n",
      "[2025-09-05 00:36:35,103: INFO: 4093871268: Dropping ID columns: ['user_id']]\n",
      "[2025-09-05 00:36:35,169: INFO: 4093871268: Processing categorical columns: ['gender', 'fitness_level', 'performance_level', 'activity_type', 'sleep_stage', 'date']]\n",
      "[2025-09-05 00:36:35,191: INFO: 4093871268: Column 'gender': 2 unique values]\n",
      "[2025-09-05 00:36:35,223: INFO: 4093871268: Column 'fitness_level': 3 unique values]\n",
      "[2025-09-05 00:36:35,250: INFO: 4093871268: Column 'performance_level': 3 unique values]\n",
      "[2025-09-05 00:36:35,284: INFO: 4093871268: Column 'activity_type': 6 unique values]\n",
      "[2025-09-05 00:36:35,315: INFO: 4093871268: Column 'sleep_stage': 3 unique values]\n",
      "[2025-09-05 00:36:35,382: INFO: 4093871268: Column 'date': 76104 unique values]\n",
      "[2025-09-05 00:36:35,384: INFO: 4093871268: Applying target encoding to: ['date']]\n",
      "[2025-09-05 00:36:35,990: INFO: 4093871268: Applying one-hot encoding to: ['gender', 'fitness_level', 'performance_level', 'activity_type', 'sleep_stage']]\n",
      "[2025-09-05 00:36:37,117: INFO: 4093871268: Final feature preparation complete - Train: (226800, 28), Test: (75600, 28)]\n",
      "[2025-09-05 00:36:37,119: INFO: 4093871268: Training HeartRatePredictor with features shape: (226800, 28)]\n",
      "=== Regression Results ===\n",
      "Train RMSE: 4.0151\n",
      "Test RMSE: 8.2909\n",
      "Train MAE: 2.9742\n",
      "Test MAE: 6.6035\n",
      "Test R²: 0.8896\n",
      "[2025-09-05 00:39:20,780: INFO: 4093871268: Model artifacts saved at: artifacts\\model_trainer\\HeartRatePredictor.joblib]\n",
      "[2025-09-05 00:39:20,797: INFO: 4093871268: Test data saved at: artifacts\\model_trainer\\HeartRatePredictor_test_data.joblib]\n",
      "[2025-09-05 00:39:20,822: INFO: 4093871268: Feature importance saved at: artifacts\\model_trainer\\HeartRatePredictor_feature_importance.csv]\n",
      "[2025-09-05 00:39:20,824: INFO: 4093871268: === Top 10 Important Features ===]\n",
      "[2025-09-05 00:39:20,825: INFO: 4093871268: activity_intensity: 0.4397]\n",
      "[2025-09-05 00:39:20,827: INFO: 4093871268: steps_5min: 0.3142]\n",
      "[2025-09-05 00:39:20,828: INFO: 4093871268: resting_hr: 0.0915]\n",
      "[2025-09-05 00:39:20,829: INFO: 4093871268: date_encoded: 0.0671]\n",
      "[2025-09-05 00:39:20,830: INFO: 4093871268: stress_score: 0.0205]\n",
      "[2025-09-05 00:39:20,832: INFO: 4093871268: activity_type_walking: 0.0130]\n",
      "[2025-09-05 00:39:20,834: INFO: 4093871268: hrv_rmssd: 0.0123]\n",
      "[2025-09-05 00:39:20,835: INFO: 4093871268: performance_level_low: 0.0100]\n",
      "[2025-09-05 00:39:20,837: INFO: 4093871268: max_hr: 0.0059]\n",
      "[2025-09-05 00:39:20,840: INFO: 4093871268: age: 0.0055]\n",
      "[2025-09-05 00:39:20,843: INFO: 4093871268: Model training completed successfully for HeartRatePredictor]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samit\\AppData\\Local\\Temp\\ipykernel_8704\\4093871268.py:25: DtypeWarning: Columns (73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv(train_path)\n",
      "C:\\Users\\samit\\AppData\\Local\\Temp\\ipykernel_8704\\4093871268.py:26: DtypeWarning: Columns (73) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  test_data = pd.read_csv(test_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-05 00:39:45,030: INFO: 4093871268: Data loaded for AnomalyDetector - Train: (226800, 336), Test: (75600, 336)]\n",
      "[2025-09-05 00:39:45,128: INFO: 4093871268: Processing categorical columns: ['activity_type', 'sleep_stage', 'date']]\n",
      "[2025-09-05 00:39:45,162: INFO: 4093871268: Column 'activity_type': 6 unique values]\n",
      "[2025-09-05 00:39:45,197: INFO: 4093871268: Column 'sleep_stage': 3 unique values]\n",
      "[2025-09-05 00:39:45,257: INFO: 4093871268: Column 'date': 76104 unique values]\n",
      "[2025-09-05 00:39:45,259: INFO: 4093871268: Applying target encoding to: ['date']]\n",
      "[2025-09-05 00:39:45,736: INFO: 4093871268: Applying one-hot encoding to: ['activity_type', 'sleep_stage']]\n",
      "[2025-09-05 00:39:46,715: INFO: 4093871268: Final feature preparation complete - Train: (226800, 20), Test: (75600, 20)]\n",
      "[2025-09-05 00:39:46,719: INFO: 4093871268: Training AnomalyDetector with features shape: (226800, 20)]\n",
      "[2025-09-05 00:39:46,721: ERROR: 4093871268: Error in model training: LGBMClassifier.fit() missing 1 required positional argument: 'y']\n",
      "Error occurred: LGBMClassifier.fit() missing 1 required positional argument: 'y'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LGBMClassifier.fit() missing 1 required positional argument: 'y'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     12\u001b[39m     hr_model, hr_predictions = hr_trainer.train_model()   \n\u001b[32m     14\u001b[39m     anomaly_trainer = ModelTrainer(\n\u001b[32m     15\u001b[39m         config=model_trainer_config, \n\u001b[32m     16\u001b[39m         schema_config=config_manager.schema,  \n\u001b[32m     17\u001b[39m         params=config_manager.params,         \n\u001b[32m     18\u001b[39m         model_name=\u001b[33m\"\u001b[39m\u001b[33mAnomalyDetector\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     anomaly_model, anomaly_predictions = \u001b[43manomaly_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 335\u001b[39m, in \u001b[36mModelTrainer.train_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    334\u001b[39m     logger.error(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError in model training: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 318\u001b[39m, in \u001b[36mModelTrainer.train_model\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    316\u001b[39m     model.fit(train_x, train_y)\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_name == \u001b[33m\"\u001b[39m\u001b[33mHeartRatePredictor\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: LGBMClassifier.fit() missing 1 required positional argument: 'y'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    \n",
    "    config_manager = ConfigurationManager()\n",
    "    model_trainer_config = config_manager.get_model_trainer_config()\n",
    "    \n",
    "    hr_trainer = ModelTrainer(\n",
    "        config=model_trainer_config, \n",
    "        schema_config=config_manager.schema,  \n",
    "        params=config_manager.params,         \n",
    "        model_name=\"HeartRatePredictor\"\n",
    "    )\n",
    "    hr_model, hr_predictions = hr_trainer.train_model()   \n",
    "    \n",
    "    anomaly_trainer = ModelTrainer(\n",
    "        config=model_trainer_config, \n",
    "        schema_config=config_manager.schema,  \n",
    "        params=config_manager.params,         \n",
    "        model_name=\"AnomalyDetector\"\n",
    "    )\n",
    "    anomaly_model, anomaly_predictions = anomaly_trainer.train_model()\n",
    "         \n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
